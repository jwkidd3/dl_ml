{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Linguistic Feature Exploration"],"metadata":{"id":"-VKm6hR7C1mk"}},{"cell_type":"markdown","source":["## Syntax Analysis\n","\n","In this example, spaCy is used to perform syntax analysis on a sentence. Syntax refers to the arrangement of words in a sentence to make grammatical sense.\n","\n","\n","This script processes a sentence and prints out the attributes of each token (word). These attributes include the lemma (base form of the word), part of speech (POS), detailed part-of-speech tag, syntactic dependency (how words are related to each other), the shape of the word, whether it's alphabetical, and whether it's considered a stopword.\n","\n","\n"],"metadata":{"id":"0ajWMdsaF5Ou"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy model\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"x_ETc-F1C9kI","executionInfo":{"status":"ok","timestamp":1699164049592,"user_tz":-360,"elapsed":482,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Process a sentence\n","sentence = \"The quick brown fox jumps over the lazy dog.\"\n","doc = nlp(sentence)\n","\n","# Token attributes for syntax analysis\n","print(\"Text\\tLemma\\tPOS\\tTag\\tDep\\tShape\\tis_alpha\\tis_stop\")\n","for token in doc:\n","    print(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\\t{token.dep_}\\t{token.shape_}\\t{token.is_alpha}\\t{token.is_stop}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_UQCuywDACA","executionInfo":{"status":"ok","timestamp":1699164049592,"user_tz":-360,"elapsed":2,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"cf93627f-59ff-44cc-ec71-7bb9b13c3955"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Text\tLemma\tPOS\tTag\tDep\tShape\tis_alpha\tis_stop\n","The\tthe\tDET\tDT\tdet\tXxx\tTrue\tTrue\n","quick\tquick\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n","brown\tbrown\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n","fox\tfox\tNOUN\tNN\tnsubj\txxx\tTrue\tFalse\n","jumps\tjump\tVERB\tVBZ\tROOT\txxxx\tTrue\tFalse\n","over\tover\tADP\tIN\tprep\txxxx\tTrue\tTrue\n","the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n","lazy\tlazy\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n","dog\tdog\tNOUN\tNN\tpobj\txxx\tTrue\tFalse\n",".\t.\tPUNCT\t.\tpunct\t.\tFalse\tFalse\n"]}]},{"cell_type":"markdown","source":["## Semantic Similarity\n","\n","After processing two sentences with spaCy, the script calculates a similarity score based on the semantic meaning of the words in each sentence. The score is a number between 0 and 1, where 1 means identical text"],"metadata":{"id":"aSNiZ7ToDIBk"}},{"cell_type":"code","source":["# Process some text\n","doc1 = nlp(\"I like salty fries and hamburgers.\")\n","doc2 = nlp(\"Burgers and Fries are my favorties.\")\n","\n","# Get the similarity between two docs\n","similarity = doc1.similarity(doc2)\n","print(f\"Similarity: {similarity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G30MZJWVDPA7","executionInfo":{"status":"ok","timestamp":1699163960681,"user_tz":-360,"elapsed":2,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"dec51687-f249-44ef-92b0-60d41b348e31"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarity: 0.6811105901346705\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-84498f7e1267>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  similarity = doc1.similarity(doc2)\n"]}]},{"cell_type":"markdown","source":["## Structure Analysis with TextBlob\n","\n","TextBlob is used here to parse a paragraph and extract noun phrases. Noun phrases give us insight into the key subjects and topics within the text, which is a structural component of language understanding"],"metadata":{"id":"eOOCzg3MEkYW"}},{"cell_type":"code","source":["import nltk\n","\n","# Download packages from nltk\n","nltk.download('all')"],"metadata":{"id":"zjvx95NiFe6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Example paragraph\n","paragraph = \"\"\"\n","Natural Language Processing enables\n","the computer to understand human language.\n","It is a field of study focused on making\n","sense of language and getting the computer\n","to perform useful tasks utilizing the\n","language data.\n","\"\"\"\n","\n","# Create a TextBlob object\n","blob = TextBlob(paragraph)\n","\n","# Noun Phrase extraction for structure analysis\n","print(\"Noun Phrases:\")\n","for np in blob.noun_phrases:\n","    print(np)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYBgcMY4Em5s","executionInfo":{"status":"ok","timestamp":1699164334356,"user_tz":-360,"elapsed":469,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"0bdf0349-abef-43f7-ff27-d512a9d774bd"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Noun Phrases:\n","language processing\n","human language\n","useful tasks\n","language data\n"]}]},{"cell_type":"markdown","source":["# Text Preprocessing"],"metadata":{"id":"-h19kBrMJiv7"}},{"cell_type":"markdown","source":["## Stopword removal\n","\n","This script uses the NLTK library to filter out stopwords from a sample sentence. It first tokenizes the sentence into words and then removes words that are present in the predefined list of English stopwords."],"metadata":{"id":"XhCH3PAKJoHj"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","text = \"This is a sample sentence, showing off the stop words filtration.\"\n","stop_words = set(stopwords.words('english'))\n","\n","word_tokens = word_tokenize(text)\n","\n","filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n","filtered_text = \" \".join(filtered_text)\n","\n","print(filtered_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogHNhtZJJijK","executionInfo":{"status":"ok","timestamp":1699165407082,"user_tz":-360,"elapsed":1836,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"4ebf5cae-7608-4611-c360-085a337abb17"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["sample sentence , showing stop words filtration .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## Punctuation Removal using String Library\n","\n","This code removes punctuation using Python's built-in string library. It translates every punctuation mark into a None character, effectively removing them from the text.\n","\n"],"metadata":{"id":"r_SsXXiJJrfh"}},{"cell_type":"code","source":["import string\n","\n","text = \"This is a sample sentence, with some punctuation!\"\n","clean_text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","print(clean_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWJCRJpfJvz2","executionInfo":{"status":"ok","timestamp":1699165446144,"user_tz":-360,"elapsed":449,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"e5346ec4-f463-49e5-af2d-a50657e275ab"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a sample sentence with some punctuation\n"]}]},{"cell_type":"markdown","source":["## POS tagging with NLTK\n","\n","This code tokenizes a sentence into words and then uses NLTK's pos_tag function to assign part-of-speech tags to each token.\n","\n"],"metadata":{"id":"qzO8oOA2Jy85"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('averaged_perceptron_tagger')\n","\n","text = \"The quick brown fox jumps over the lazy dog\"\n","word_tokens = word_tokenize(text)\n","\n","pos_tags = nltk.pos_tag(word_tokens)\n","\n","print(pos_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZcXVxLSJ0yL","executionInfo":{"status":"ok","timestamp":1699165460467,"user_tz":-360,"elapsed":453,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"ea1dbd05-7399-4599-f650-dea388aca7f3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"stream","name":"stdout","text":["[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"]}]},{"cell_type":"markdown","source":["## Tokenization using NLTK\n","\n","This script uses NLTK's word_tokenize function to split the text into a list of words. This process is known as tokenization, which is the first step in text preprocessing for NLP"],"metadata":{"id":"86RP2NkYJ4dO"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","\n","text = \"Natural language processing is a field of computer science, AI\"\n","tokens = word_tokenize(text)\n","\n","print(tokens)\n"],"metadata":{"id":"Rr036hsGJ7PW","executionInfo":{"status":"ok","timestamp":1699165486628,"user_tz":-360,"elapsed":519,"user":{"displayName":"Emon Sarker","userId":"03285385557850436992"}},"outputId":"590d0ff2-e897-46af-bc52-91853968f07d","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'AI']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]}]}