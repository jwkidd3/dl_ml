{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Introduction to Loss Functions\n","Loss functions quantify how well the predicted output of the model matches the true output. We'll explore several loss functions in this section.\n"],"metadata":{"id":"0ajWMdsaF5Ou"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# Mean Squared Error Loss\n","mse_loss = nn.MSELoss()\n","\n","# Cross-Entropy Loss\n","ce_loss = nn.CrossEntropyLoss()\n","\n","# Hinge Loss\n","hinge_loss = nn.HingeEmbeddingLoss()\n"],"metadata":{"id":"Krkn4-yS0CYG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Learning Rates\n","The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network concerning the loss gradient.\n"],"metadata":{"id":"EoagGjuV0E9n"}},{"cell_type":"code","source":["learning_rates = [0.1, 0.01, 0.001]\n"],"metadata":{"id":"A8AmNog60GdF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Optimizers\n","Optimizers are algorithms used to minimize the loss function by adjusting model parameters. We'll implement various optimizers and observe their impact on model training.\n"],"metadata":{"id":"t3hhpa9o0JTV"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","model = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 1))\n","\n","# Implementing various optimizers\n","optimizers = {\n","    'SGD': optim.SGD(model.parameters(), lr=0.01),\n","    'Adam': optim.Adam(model.parameters(), lr=0.001),\n","    'RMSprop': optim.RMSprop(model.parameters(), lr=0.01)\n","}\n"],"metadata":{"id":"_cYPRhbE0Kwq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Finetuning Learning Rate, Optimizers, and Loss Functions\n","Fine-tuning involves adjusting the hyperparameters of the model to improve its performance. We'll experiment with different combinations of learning rates, optimizers, and loss functions to observe their effect on model convergence and accuracy.\n"],"metadata":{"id":"C41Kmb8n0N_1"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score\n","\n","# Dummy data (10 samples, 10 features)\n","X = torch.randn(10, 10)\n","y = torch.randint(0, 2, (10,))  # Binary target variable\n","\n","# Define your model architecture\n","class SimpleModel(nn.Module):\n","    def __init__(self):\n","        super(SimpleModel, self).__init__()\n","        self.fc1 = nn.Linear(10, 5)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(5, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# Define the loss functions\n","bce_loss = nn.BCELoss()\n","\n","# Define the learning rates\n","learning_rates = [0.1, 0.01, 0.001]\n","\n","# Define the optimizer classes, not instances\n","optimizers_classes = {\n","    'SGD': optim.SGD,\n","    'Adam': optim.Adam,\n","    'RMSprop': optim.RMSprop\n","}\n","\n","# Define the training and evaluation function\n","def train_and_evaluate(optimizer_class, loss_function, learning_rate):\n","    # Creating an instance of the model\n","    model_instance = SimpleModel()\n","\n","    # Creating an instance of the optimizer\n","    optimizer = optimizer_class(model_instance.parameters(), lr=learning_rate)\n","\n","    # Training code\n","    epochs = 100\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        output = model_instance(X)\n","        loss = loss_function(output.squeeze(), y.float())\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Model evaluation\n","    with torch.no_grad():\n","        output = model_instance(X)\n","        predictions = (output.squeeze() > 0.5).long()\n","        accuracy = accuracy_score(y.numpy(), predictions.numpy())\n","        print(f'Accuracy: {accuracy * 100:.2f}%')\n","\n","# Loop through combinations of optimizer classes, loss functions, and learning rates to train and evaluate the model\n","for optimizer_name, optimizer_class in optimizers_classes.items():\n","    for lr in learning_rates:\n","        print(f\"\\nTraining with {optimizer_name}, learning rate: {lr}, loss function: {bce_loss.__class__.__name__}\")\n","        train_and_evaluate(optimizer_class, bce_loss, lr)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1TwwHt9oLvC","outputId":"57d26cb5-1cd0-475f-eef3-f06d08b7c2df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training with SGD, learning rate: 0.1, loss function: BCELoss\n","Accuracy: 100.00%\n","\n","Training with SGD, learning rate: 0.01, loss function: BCELoss\n","Accuracy: 30.00%\n","\n","Training with SGD, learning rate: 0.001, loss function: BCELoss\n","Accuracy: 30.00%\n","\n","Training with Adam, learning rate: 0.1, loss function: BCELoss\n","Accuracy: 100.00%\n","\n","Training with Adam, learning rate: 0.01, loss function: BCELoss\n","Accuracy: 100.00%\n","\n","Training with Adam, learning rate: 0.001, loss function: BCELoss\n","Accuracy: 80.00%\n","\n","Training with RMSprop, learning rate: 0.1, loss function: BCELoss\n","Accuracy: 70.00%\n","\n","Training with RMSprop, learning rate: 0.01, loss function: BCELoss\n","Accuracy: 100.00%\n","\n","Training with RMSprop, learning rate: 0.001, loss function: BCELoss\n","Accuracy: 60.00%\n"]}]},{"cell_type":"markdown","source":["# Another walk-through of deep learning"],"metadata":{"id":"YJnbTHJ18nP5"}},{"cell_type":"code","source":[],"metadata":{"id":"D4kz3GqB8uzu"},"execution_count":null,"outputs":[]}]}