{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Linguistic Feature Exploration"],"metadata":{"id":"-VKm6hR7C1mk"}},{"cell_type":"markdown","source":["## Syntax Analysis\n","\n","In this example, spaCy is used to perform syntax analysis on a sentence. Syntax refers to the arrangement of words in a sentence to make grammatical sense.\n","\n","\n","This script processes a sentence and prints out the attributes of each token (word). These attributes include the lemma (base form of the word), part of speech (POS), detailed part-of-speech tag, syntactic dependency (how words are related to each other), the shape of the word, whether it's alphabetical, and whether it's considered a stopword.\n","\n","\n"],"metadata":{"id":"0ajWMdsaF5Ou"}},{"cell_type":"code","source":["import spacy\n","\n","# Load the spaCy model\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"x_ETc-F1C9kI","executionInfo":{"status":"ok","timestamp":1700480997481,"user_tz":-360,"elapsed":12666,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_O8JJRBSkRkU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process a sentence\n","sentence = \"The quick brown fox runs faster the lazy dog.\"\n","doc = nlp(sentence)\n","\n","# Token attributes for syntax analysis\n","print(\"Text\\tLemma\\tPOS\\tTag\\tDep\\tShape\\tis_alpha\\tis_stop\")\n","for token in doc:\n","    print(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\\t{token.dep_}\\t{token.shape_}\\t{token.is_alpha}\\t{token.is_stop}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_UQCuywDACA","outputId":"f4fee4ce-db5b-4813-8087-8288bab9fdf7","executionInfo":{"status":"ok","timestamp":1700481040692,"user_tz":-360,"elapsed":13,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Text\tLemma\tPOS\tTag\tDep\tShape\tis_alpha\tis_stop\n","The\tthe\tDET\tDT\tdet\tXxx\tTrue\tTrue\n","quick\tquick\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n","brown\tbrown\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n","fox\tfox\tNOUN\tNN\tnsubj\txxx\tTrue\tFalse\n","runs\trun\tVERB\tVBZ\tROOT\txxxx\tTrue\tFalse\n","faster\tfast\tADV\tRBR\tadvmod\txxxx\tTrue\tFalse\n","the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n","lazy\tlazy\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n","dog\tdog\tNOUN\tNN\tdobj\txxx\tTrue\tFalse\n",".\t.\tPUNCT\t.\tpunct\t.\tFalse\tFalse\n"]}]},{"cell_type":"markdown","source":["## Semantic Similarity\n","\n","After processing two sentences with spaCy, the script calculates a similarity score based on the semantic meaning of the words in each sentence. The score is a number between 0 and 1, where 1 means identical text"],"metadata":{"id":"aSNiZ7ToDIBk"}},{"cell_type":"code","source":["doc1 = nlp(\"Burgers and Fries are not my favorties.\")\n","doc2 = nlp(\"Burgers and Fries are my favorties.\")"],"metadata":{"id":"ZZ2VHxljk1_k","executionInfo":{"status":"ok","timestamp":1700481395304,"user_tz":-360,"elapsed":12,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["doc1.similarity(doc2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nf2agcSAk3Ij","executionInfo":{"status":"ok","timestamp":1700481395956,"user_tz":-360,"elapsed":8,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}},"outputId":"45a15e22-543f-4b2e-8935-888df29ae69f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-d3bf01ccd94a>:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  doc1.similarity(doc2)\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9101511475178125"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Process some text\n","\n","\n","# Get the similarity between two docs\n","similarity = doc1.similarity(doc2)\n","print(f\"Similarity: {similarity}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G30MZJWVDPA7","outputId":"f01bedd6-868e-448b-f221-ec128b0887fb","executionInfo":{"status":"ok","timestamp":1700458361638,"user_tz":-360,"elapsed":4,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Similarity: 0.2030433545638997\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-7db0fcf89bc4>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  similarity = doc1.similarity(doc2)\n"]}]},{"cell_type":"markdown","source":["## Structure Analysis with TextBlob\n","\n","TextBlob is used here to parse a paragraph and extract noun phrases. Noun phrases give us insight into the key subjects and topics within the text, which is a structural component of language understanding"],"metadata":{"id":"eOOCzg3MEkYW"}},{"cell_type":"code","source":["import nltk\n","\n","# Download packages from nltk\n","nltk.download('all')"],"metadata":{"id":"zjvx95NiFe6k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob\n","\n","# Example paragraph\n","paragraph = \"\"\"\n","Natural Language Processing enables\n","the computer to understand human language.\n","It is a field of study focused on making\n","sense of language and getting the computer\n","to perform useful tasks utilizing the\n","language data.\n","\"\"\"\n","\n","# Create a TextBlob object\n","blob = TextBlob(paragraph)\n","\n","# Noun Phrase extraction for structure analysis\n","print(\"Noun Phrases:\")\n","for np in blob.noun_phrases:\n","    print(np)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYBgcMY4Em5s","outputId":"0bdf0349-abef-43f7-ff27-d512a9d774bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Noun Phrases:\n","language processing\n","human language\n","useful tasks\n","language data\n"]}]},{"cell_type":"markdown","source":["# Text Preprocessing"],"metadata":{"id":"-h19kBrMJiv7"}},{"cell_type":"markdown","source":["## Stopword removal\n","\n","This script uses the NLTK library to filter out stopwords from a sample sentence. It first tokenizes the sentence into words and then removes words that are present in the predefined list of English stopwords."],"metadata":{"id":"XhCH3PAKJoHj"}},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","text = \"This is a sample sentence, showing off the stop words filtration.\"\n","stop_words = set(stopwords.words('english'))\n","\n","word_tokens = word_tokenize(text)\n","\n","filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n","filtered_text = \" \".join(filtered_text)\n","\n","print(filtered_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogHNhtZJJijK","outputId":"1c7c3b8c-ae22-49d5-ccdb-6716b5dc0940","executionInfo":{"status":"ok","timestamp":1700482678947,"user_tz":-360,"elapsed":725,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["sample sentence , showing stop words filtration .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"code","source":["stop_words.discard('a')"],"metadata":{"id":"i6pGhxJtm-bv","executionInfo":{"status":"ok","timestamp":1700482664465,"user_tz":-360,"elapsed":391,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["## Punctuation Removal using String Library\n","\n","This code removes punctuation using Python's built-in string library. It translates every punctuation mark into a None character, effectively removing them from the text.\n","\n"],"metadata":{"id":"r_SsXXiJJrfh"}},{"cell_type":"code","source":["import string\n","\n","text = \"This is a sample sentence, with some punctuation!\"\n","clean_text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","print(clean_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWJCRJpfJvz2","outputId":"e5346ec4-f463-49e5-af2d-a50657e275ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a sample sentence with some punctuation\n"]}]},{"cell_type":"markdown","source":["## POS tagging with NLTK\n","\n","This code tokenizes a sentence into words and then uses NLTK's pos_tag function to assign part-of-speech tags to each token.\n","\n"],"metadata":{"id":"qzO8oOA2Jy85"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('averaged_perceptron_tagger')\n","\n","text = \"This is a dog\"\n","word_tokens = word_tokenize(text)\n","\n","pos_tags = nltk.pos_tag(word_tokens)\n","\n","print(pos_tags)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZcXVxLSJ0yL","outputId":"705e11a4-6a91-4b64-c420-4e4c0beb9236","executionInfo":{"status":"ok","timestamp":1700482112973,"user_tz":-360,"elapsed":415,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"stream","name":"stdout","text":["[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('dog', 'NN')]\n"]}]},{"cell_type":"markdown","source":["## Tokenization using NLTK\n","\n","This script uses NLTK's word_tokenize function to split the text into a list of words. This process is known as tokenization, which is the first step in text preprocessing for NLP"],"metadata":{"id":"86RP2NkYJ4dO"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt')\n","\n","text = \"Natural language processing is a field of computer science, AI\"\n","tokens = word_tokenize(text)\n","\n","print(tokens)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rr036hsGJ7PW","outputId":"590d0ff2-e897-46af-bc52-91853968f07d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'AI']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","ps = PorterStemmer()\n","ps\n"],"metadata":{"id":"T9o-iBpERrCA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700482259672,"user_tz":-360,"elapsed":414,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}},"outputId":"1628a35a-e872-4db6-a4ef-16fb2b37dce5"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["<PorterStemmer>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["#Stemming\n","\n","example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n","print('Stemming word Example')\n","print(example_words)\n","print('\\n')\n","print('After Stemming, Words are')\n","# Next, we can easily stem by doing something like:\n","for w in example_words:\n","    print(ps.stem(w))"],"metadata":{"id":"tRHYWvOzRfXZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700482506607,"user_tz":-360,"elapsed":11,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}},"outputId":"693bd079-2d44-43c2-ac76-45cee6f9d51c"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Stemming word Example\n","['python', 'pythoner', 'pythoning', 'pythoned', 'pythonly']\n","\n","\n","After Stemming, Words are\n","python\n","python\n","python\n","python\n","pythonli\n"]}]},{"cell_type":"code","source":["new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n","print('Another Example')\n","print(new_text)\n","print('\\n')\n","\n","print('For each word, stemming done as follows')\n","# Word Tokenizer\n","words = word_tokenize(new_text)\n","# For each word, stemming done\n","for w in words:\n","    print(ps.stem(w))"],"metadata":{"id":"4uEGwXUjRnCw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700482353477,"user_tz":-360,"elapsed":387,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}},"outputId":"772703b0-6ecc-4e1f-f2d0-010426f9400d"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Another Example\n","It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\n","\n","\n","For each word, stemming done as follows\n","it\n","is\n","import\n","to\n","by\n","veri\n","pythonli\n","while\n","you\n","are\n","python\n","with\n","python\n",".\n","all\n","python\n","have\n","python\n","poorli\n","at\n","least\n","onc\n",".\n"]}]},{"cell_type":"code","source":["\n","lemmatizer = WordNetLemmatizer()\n","\n","print(lemmatizer.lemmatize(\"python\"))\n","print(lemmatizer.lemmatize(\"python\"))\n","print(lemmatizer.lemmatize(\"geese\"))\n","print(lemmatizer.lemmatize(\"rocks\"))\n","print(lemmatizer.lemmatize(\"python\"))\n","print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n","print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n","print(lemmatizer.lemmatize(\"run\"))\n","print(lemmatizer.lemmatize(\"run\",'v'))"],"metadata":{"id":"77NdkFi6R0Ta","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700482417345,"user_tz":-360,"elapsed":2832,"user":{"displayName":"Rafat Haameem","userId":"13607073244277143982"}},"outputId":"f9cf64fb-b587-4f1d-ca73-267d79e8092c"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["cat\n","cactus\n","goose\n","rock\n","python\n","good\n","best\n","run\n","run\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8aMDTrqWpnRj"},"execution_count":null,"outputs":[]}]}